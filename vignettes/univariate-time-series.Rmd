---
title: "Univariate Time Series"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{univariate-time-series}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(torch)
library(torchts)
library(rsample)
suppressMessages(library(dplyr))
library(ggplot2)
library(parsnip)
```

## Doing it with parsnip API

`parsnip` API provides quick and convenient way to train time series models based on `torch` library.

```{r preparing.data}
tarnow_temp <- 
  weather_pl %>% 
  filter(station == "TARNÃ“W") %>% 
  select(date, temp = tmax_daily)

# Splitting on training and test
data_split <- initial_time_split(tarnow_temp)
```


```{r training.parsnip.api}
# Training 
model <- 
  rnn(epochs = 10, hidden_units = 32) %>% 
  fit(temp ~ temp + index(date), 
      data = training(data_split))

prediction <-
  model %>%
  predict(new_data = testing(data_split))

View(bind_cols(
  testing(data_split), prediction$.pred
))
```


```{r preparing.dls}
train_ds <- 
  training(data_split) %>% 
  as_ts_dataset(Temp ~ Temp + index(Date), n_timesteps = 20, h = 1)

test_ds <- 
  testing(data_split) %>% 
  as_ts_dataset(Temp ~ Temp + index(Date), n_timesteps = 20, h = 1)

train_dl <- dataloader(train_ds, batch_size = 5)
test_dl  <- dataloader(test_ds, batch_size = 5)
```


```{r resolve.ts}
# resolve_ts_dataset() = initial_time_split + 
# resolve_ts_dataloader
# train_ds[1]

# train_ds$.data[1:20, 1, drop = FALSE]
# b <- train_dl %>% dataloader_make_iter() %>% dataloader_next()
# b
```

```{r creating.network}
model <- nn_module(
  
  initialize = function(type, input_size, hidden_size, 
                        num_layers = 1, dropout = 0) {
    
    self$type <- type
    self$num_layers <- num_layers
    
    self$rnn <-
      nn_gru(
        input_size = input_size,
        hidden_size = hidden_size,
        num_layers = num_layers,
        dropout = dropout,
        batch_first = TRUE
      )
   
    self$output <- nn_linear(hidden_size, 1)
    
  },
  
  forward = function(x) {
    
    # list of [output, hidden]
    # we use the output, which is of size (batch_size, n_timesteps, hidden_size)
    x <- self$rnn(x)[[1]]
    
    # from the output, we only want the final timestep
    # shape now is (batch_size, hidden_size)
    x <- x[ , dim(x)[2], ]
    
    # feed this to a single output neuron
    # final shape then is (batch_size, 1)
    x %>% self$output() 
  }
  
)

net <- model("gru", 1, 32)
```

```{r training}
optimizer <- optim_adam(net$parameters, lr = 0.001)

num_epochs <- 30

train_batch <- function(b) {
  
  optimizer$zero_grad()
  output <- net(b$x)
  target <- b$y
  
  loss <- nnf_mse_loss(output, target)
  loss$backward()
  optimizer$step()
  
  loss$item()
}

valid_batch <- function(b) {
  
  output <- net(b$x)
  target <- b$y
  
  loss <- nnf_mse_loss(output, target)
  loss$item()
  
}

num_epochs <- 10 

# for (epoch in 1:num_epochs) {
#   
#   net$train()
#   train_loss <- c()
#   
#   coro::loop(for (b in train_dl) {
#     loss <- train_batch(b)
#     train_loss <- c(train_loss, loss)
#   })
#   
#   cat(sprintf("\nEpoch %d, training: loss: %3.5f \n", epoch, mean(train_loss)))
#   
#   net$eval()
#   valid_loss <- c()
#   
#   # coro::loop(for (b in valid_dl) {
#   #   loss <- valid_batch(b)
#   #   valid_loss <- c(valid_loss, loss)
#   # })
#   # 
#   # cat(sprintf("\nEpoch %d, validation: loss: %3.5f \n", epoch, mean(valid_loss)))
# }

```

```{r forecasting}
# net$eval()
# 
preds <- rep(NA, 20)

coro::loop(for (b in test_dl) {
  print(b$x)
  # output <- net(b$x)
  # preds <- c(preds, output %>% as.numeric())
})

```

```{r compare.with.ground.truth}

comparison <-
  testing(data_split) %>%
  mutate(forecast = prediction$.pred) %>%
  mutate(Date = lubridate::as_date(Date)) %>%
  tidyr::pivot_longer(c(Temp, forecast))

plt <-
  ggplot(comparison) +
  geom_line(aes(Date, value, col = name))

plotly::ggplotly(plt)

```

