---
title: "Univariate Time Series"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{univariate-time-series}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(torch)
library(torchts)
library(rsample)
suppressMessages(library(dplyr))
library(ggplot2)
library(parsnip)
```

```{r preparing.data}
data_set <-
  read.csv("https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv")

# Splitting on training and test
data_split <- initial_time_split(data_set)
# training(data_split)

train_ds <- 
  training(data_split) %>% 
  as_ts_dataset(Temp ~ Temp + index(Date), n_timesteps = 20, h = 1)

test_ds <- 
  testing(data_split) %>% 
  as_ts_dataset(Temp ~ Temp + index(Date), n_timesteps = 20, h = 1)

train_dl <- dataloader(train_ds, batch_size = 5)
test_dl  <- dataloader(test_ds, batch_size = 5)
```

```{r resolve.ts}
# resolve_ts_dataset() = initial_time_split + 
# resolve_ts_dataloader
# train_ds[1]

# train_ds$.data[1:20, 1, drop = FALSE]
# b <- train_dl %>% dataloader_make_iter() %>% dataloader_next()
# b
```

```{r creating.network}
model <- nn_module(
  
  initialize = function(type, input_size, hidden_size, 
                        num_layers = 1, dropout = 0) {
    
    self$type <- type
    self$num_layers <- num_layers
    
    self$rnn <-
      nn_gru(
        input_size = input_size,
        hidden_size = hidden_size,
        num_layers = num_layers,
        dropout = dropout,
        batch_first = TRUE
      )
   
    self$output <- nn_linear(hidden_size, 1)
    
  },
  
  forward = function(x) {
    
    # list of [output, hidden]
    # we use the output, which is of size (batch_size, n_timesteps, hidden_size)
    x <- self$rnn(x)[[1]]
    
    # from the output, we only want the final timestep
    # shape now is (batch_size, hidden_size)
    x <- x[ , dim(x)[2], ]
    
    # feed this to a single output neuron
    # final shape then is (batch_size, 1)
    x %>% self$output() 
  }
  
)

net <- model("gru", 1, 32)
```

```{r training}
optimizer <- optim_adam(net$parameters, lr = 0.001)

num_epochs <- 30

train_batch <- function(b) {
  
  optimizer$zero_grad()
  output <- net(b$x)
  target <- b$y
  
  loss <- nnf_mse_loss(output, target)
  loss$backward()
  optimizer$step()
  
  loss$item()
}

valid_batch <- function(b) {
  
  output <- net(b$x)
  target <- b$y
  
  loss <- nnf_mse_loss(output, target)
  loss$item()
  
}

num_epochs <- 10 

# for (epoch in 1:num_epochs) {
#   
#   net$train()
#   train_loss <- c()
#   
#   coro::loop(for (b in train_dl) {
#     loss <- train_batch(b)
#     train_loss <- c(train_loss, loss)
#   })
#   
#   cat(sprintf("\nEpoch %d, training: loss: %3.5f \n", epoch, mean(train_loss)))
#   
#   net$eval()
#   valid_loss <- c()
#   
#   # coro::loop(for (b in valid_dl) {
#   #   loss <- valid_batch(b)
#   #   valid_loss <- c(valid_loss, loss)
#   # })
#   # 
#   # cat(sprintf("\nEpoch %d, validation: loss: %3.5f \n", epoch, mean(valid_loss)))
# }

```
```{r forecasting}
# net$eval()
# 
# preds <- rep(NA, 20)
# 
# coro::loop(for (b in test_dl) {
#   output <- net(b$x)
#   preds <- c(preds, output %>% as.numeric())
# })

```

```{r compare.with.ground.truth}

# comparison <- 
#   testing(data_split) %>% 
#   mutate(forecast = preds) %>% 
#   mutate(Date = lubridate::as_date(Date)) %>% 
#   tidyr::pivot_longer(c(Temp, forecast))
# 
# plt <- 
#   ggplot(comparison) +
#   geom_line(aes(Date, value, col = name))
# 
# plotly::ggplotly(plt)

```

## pre-parsnip API

```{r pre.parsnip.api}
# debugonce(recurrent_network_fit_formula)

# fitted_model <-
#   recurrent_network_fit_formula(
#     formula     = Temp ~ Temp + index(Date),
#     data        = training(data_split),
#     n_timesteps = 20,
#     h           = 1,
#     n_epochs    = 3 
#   )
```

## parsnip API

```{r parsnip.api}
model <- 
  recurrent_network(epochs = 3) %>% 
  fit(Temp ~ Temp + index(Date), 
      data = training(data_split))

prediction <- 
  model %>% 
  predict(new_data = testing(data_split))
```
