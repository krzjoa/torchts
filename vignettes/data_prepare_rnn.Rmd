---
title: "Preparing data for RNN models"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Preparing data for RNN models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(magrittr)
library(dplyr)
library(torch)
library(torchts)
```

## Downloading M5 challenge data set
```{r downloading.data}
# Downloading Walmart data
# Walmart data
```

The M5 challenge data set is quite large, so it will be good to reduce
the input data frame.


## Data Wrangling
```{r preparing.data}
walmart_foods_ca1_prepared <-
  fst::read_fst("../data/walmart/walmart_foods_ca1_prepared.fst")

experiment_data <-
  walmart_foods_ca1_prepared %>%
  filter(dept_id == "FOODS_1") %>%
  select(-snap_TX, -snap_WI) %>%
  select(item_id, d, wm_yr_wk, id, dept_id, value,
         date, wday, month, year, event_name_1,
         event_type_1, event_name_2, event_type_2, snap_CA,
         sell_price)

experiment_data <-
  experiment_data %>%
  mutate(date = lubridate::as_date(date)) %>%
  mutate(across(where(is.factor), as.character))

experiment_data <-
  experiment_data %>%
  select(item_id, date,value, wday,
         month, year, snap_CA, sell_price) %>% 
    arrange(item_id, date)
```

We assume that we want to create a **three-dimensional tensor**, the each dimension of 
presents:

* **item**
* **time steps**
* **features**

This assumption makes us to keep the data "complete", i.e. every single item time series 
has to have the same **length**. The first dimension is "free", and the completeness of 
the last dimension is guaranteed by the data.frame structure itself (each row has the same number of columns).

For simplicity's sake, we can just select a subset of items with the same series length as well as the first and last date. In such case, we'll be sure that our data are properly aligned in the tensor. Later, in a separate vignette, we'll dive into a set of methods, how to handle missing/non-aligned multiple time series when training a deep learning model.


```{r filtering.data}
data_summary <- 
  experiment_data %>% 
    group_by(item_id) %>% 
    summarise(
      len = n(),
      first_date = min(date),
      last_date  = max(date)
    ) %>% 
    ungroup() %>% 
    group_by(len, first_date, last_date) %>% 
    summarise(
      n = n(),
      item_id = list(item_id)
    ) %>% 
    arrange(desc(n))

print(head(data_summary))
```
We choose a subset consisting of 101 products - each product has:

 * lenght: 1913 days
 * first_date: 2011-01-29
 * last_date: 2016-04-24

If we check the tme difference between these two dates

```{r time.diff}
as.Date('2016-04-24') - as.Date('2011-01-29')
```
we'll can see that there are no gaps in the subset we've just selected, so 
we have not to do any additional filtering or aligning.

```{r data.subset}
selected_items <- 
  data_summary[1, ] %>% 
  pull(item_id) %>% 
  unlist()

experiment_data <- 
  experiment_data %>% 
  filter(item_id %in% selected_items)

print(nrow(experiment_data))
```
Let's take a look on the data we've already prepared.

## Transorming to tensor

```{r prepared.data}
head(experiment_data)
```
The first column, `item_id`, describes the item we already the item and the second one (`date`) - a current time step. These two columns will be used to create a data "fold", i.e. form a 3D tensor. As we mentioned above, the completness of the time moments is crucial to obtain a proper result from this transformation.

If it comes, to the rest of columns:

* `value` is a target we want to predict
* `wday` is a categorical variable
* `month` is a categorical variable
* `year` *can* be treated as categorical, but in this case we may remove this variable and introduce a counter instead
* `snap_CA` is a categorical variable
* `sell_price` is a real-valued variable

Summarizing, we have three categorical variables, which should be represented in some way.
The most efficient manner to represent categorical variables in neural network is **embedding** layer.  
In fact, it works similar to a **linear** (**dense**)  layer. The difference is that instead of performing resource-consuming dot product between weight matrix and the input one-hot encoded sparse matrix, we just use an index to select "right" row from the weight matrix.

Because of this index-based nature of embedding, we need to transform all the categorical features to 1-n to range.
```{r unique.cat}
experiment_data %>% 
  select(wday, month, snap_CA) %>% 
  sapply(unique)
```
In this case the only variable we need to recode is snap_Ca.

```{r recode.var}
experiment_data <- 
  experiment_data %>% 
  mutate(snap_CA = ifelse(snap_CA == 0, 1, 2))
```

```{r dict.size}
d_size <-
  experiment_data %>%
  select(
    wday, month,
    snap_CA
  ) %>%
  dict_size()
```

Let's transform tabular data into tensor. A good way to do it is to use 
a convenient `as_tensor` function. The first argument of the function (described as `.data`) 
is a `data.frame` object, which we want to transform into a `torch_tensor`.

```{r prepare.tensor}
X_tensor_cat <-
  experiment_data %>%
  select(
    item_id, date, wday, month,
    snap_CA
  ) %>%
  filter(item_id == "FOODS_1_001") %>% 
  as_tensor(item_id, date, dtype = torch_long())

X_tensor_rest <- 
  experiment_data %>%
  select(
    item_id, date, sell_price
  ) %>%
  filter(item_id == "FOODS_1_001") %>% 
  as_tensor(item_id, date, dtype = torch_long())

print(class(X_tensor_cat))
print(X_tensor_cat$shape)

y_tensor <-
  experiment_data %>%
  select(item_id, date, value) %>%
  filter(item_id == "FOODS_1_001") %>% 
  as_tensor(item_id, date, dtype = torch_float())
```

### Embedding for categorical variables

First, let's create **a multiple embedding module**.
A causal **embedding module** is a simple, but memory efficient operation, which allows us to transform a sequence of categorical features into sequence of embedding vectors.

```{r network}
embedding <- 
  nn_multi_embedding(
    num_embeddings = d_size,
    embedding_dim  = rep(3, length(d_size))
  )

X_tensor_cat_processed <-  embedding(X_tensor_cat)
```

### Feature concatenation

```{r concat}
X_transformed <- torch_cat(
  list(X_tensor_rest, X_tensor_cat_processed), dim = -1
)
```

```{r recurrent.layer}
# recurrent_layer <- nn_gru(12, 48)
# out <- recurrent_layer(X_transformed)
# out 
# 
# nn_linear(48, 1)(out[[1]])

simple_rnn <- nn_module(
  "nn_simple_rnn",
  initialize = function(num_embeddings, embedding_dim, rnn_input_size, output_size){
    self$embedding <- nn_multi_embedding(num_embeddings, embedding_dim)
    self$recurrent_layer <- nn_gru(rnn_input_size, output_size)
    self$linear <- nn_linear(output_size, 1)
  },
  forward = function(input_cat, input_rest){
    X_tensor_cat_processed <- self$embedding(X_tensor_cat)
    X_transformed <- torch_cat(
        list(X_tensor_rest, X_tensor_cat_processed), dim = -1
    )
    out <- self$recurrent_layer(X_transformed)
    nnf_relu(self$linear(nnf_relu(out[[1]])))
  }
)

simple_rnn_instance <- simple_rnn(d_size, rep(3, length(d_size)), 12, 48)
# simple_rnn_instance(X_tensor_cat, X_tensor_rest)
```

### Training Recurrent Neural Network

```{r train.network}

optim <- optim_adagrad(simple_rnn_instance$parameters, lr = 0.1)

for (i in 1:100) {
  simple_rnn_instance$zero_grad()
  fcast <- simple_rnn_instance(X_tensor_cat, X_tensor_rest)
  loss <- nnf_mse_loss(fcast, y_tensor)
  loss$backward()
  print(loss)
  optim$step()
}

trial <- function(optim){
  browser()
}

trial(optim_adagrad(lr = 0.1))
```

