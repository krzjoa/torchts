% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/categorical.R
\name{embedding_size}
\alias{embedding_size}
\alias{embedding_size_google}
\alias{embedding_size_fastai}
\title{Propose the length of embedding vector for each embedded feature.}
\usage{
embedding_size_google(x, max_size = 100)

embedding_size_fastai(x, max_size = 100)
}
\arguments{
\item{x}{(\code{integer}) A vector with dictionary size for each feature}
}
\value{
Proposed embedding sizes.
}
\description{
These functions returns proposed embedding sizes for each categorical feature.
They are "rule of thumbs", so the are based on empirical rather than theoretical conclusions,
and their parameters can look like "magic numbers". Nevertheless, when you don't know what embedding size
will be "optimal", it's good to start with such kind of general rules.
\itemize{
\item \strong{google}
Proposed on the \href{https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html}{Google Developer} site
\deqn{x^0.25}
\item \strong{fastai}
\deqn{1.6 * x^0.56}
}
}
\examples{
dict_sizes <- dict_size(tiny_m5)
embedding_size_google(dict_sizes)
embedding_size_fastai(dict_sizes)

}
\references{
\itemize{
\item \href{https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html}{Introducing TensorFlow Feature Columns}
\item \href{https://github.com/fastai/fastai/blob/master/fastai/tabular/model.py}{fastai - embedding size rule of thumb}
}
}
